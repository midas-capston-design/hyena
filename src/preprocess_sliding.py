#!/usr/bin/env python3
"""Sliding Window ë°©ì‹ ì „ì²˜ë¦¬ - Causal Trainingìš©"""
import json
import csv
import math
import random
from pathlib import Path
from typing import Dict, List, Tuple
from collections import deque, defaultdict
import numpy as np
import pywt

# ì •ê·œí™” ê¸°ì¤€ê°’
BASE_MAG = (-33.0, -15.0, -42.0)
COORD_CENTER = (-41.0, 0.0)
COORD_SCALE = 50.0

def normalize_mag(val: float, base: float) -> float:
    return (val - base) / 10.0

def normalize_coord(x: float, y: float) -> Tuple[float, float]:
    x_norm = (x - COORD_CENTER[0]) / COORD_SCALE
    y_norm = (y - COORD_CENTER[1]) / COORD_SCALE
    return (x_norm, y_norm)

def wavelet_denoise(signal: List[float], wavelet='db4', level=3) -> List[float]:
    """Wavelet denoising"""
    coeffs = pywt.wavedec(signal, wavelet, level=level)
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))
    denoised_coeffs = [pywt.threshold(c, uthresh, mode='soft') for c in coeffs]
    return pywt.waverec(denoised_coeffs, wavelet).tolist()

def read_nodes(path: Path) -> Tuple[Dict[int, Tuple[float, float]], set]:
    """ë…¸ë“œ ìœ„ì¹˜ ë° íšŒì „ ë…¸ë“œ ì½ê¸°"""
    positions = {}
    turn_nodes = set()
    with path.open() as f:
        reader = csv.DictReader(f)
        for row in reader:
            node_id = int(row["id"])
            x = float(row["x_m"])
            y = float(row["y_m"])
            node_type = row.get("type", "marker")
            positions[node_id] = (x, y)
            if node_type == "turn":
                turn_nodes.add(node_id)
    return positions, turn_nodes

def build_graph(positions: Dict[int, Tuple[float, float]]) -> Dict[int, List[Tuple[int, float]]]:
    """ê·¸ë˜í”„ êµ¬ì¶• - ë³µë„ êµ¬ì¡°ë§Œ ì—°ê²° (ê°™ì€ í–‰/ì—´)"""
    graph = {node: [] for node in positions}
    nodes = sorted(positions.keys())

    # ë¬¼ë¦¬ì ìœ¼ë¡œ ì—°ê²° ë¶ˆê°€ëŠ¥í•œ ìŒ (ë²½ ë“±)
    blocked_connections = {(10, 28), (28, 10), (24, 25), (25, 24)}

    for i, a in enumerate(nodes):
        for b in nodes[i+1:]:
            # ì°¨ë‹¨ëœ ì—°ê²° í™•ì¸
            if (a, b) in blocked_connections or (b, a) in blocked_connections:
                continue

            xa, ya = positions[a]
            xb, yb = positions[b]
            # Manhattan distance (ë³µë„ êµ¬ì¡°: ëŒ€ê°ì„  ì´ë™ ë¶ˆê°€)
            dist = abs(xb - xa) + abs(yb - ya)

            # 5m ì´í•˜ì´ê³  ê°™ì€ í–‰/ì—´ë§Œ ì—°ê²°
            if dist <= 5.0:
                same_row = abs(ya - yb) < 0.5  # y ì°¨ì´ < 0.5m
                same_col = abs(xa - xb) < 0.5  # x ì°¨ì´ < 0.5m

                if same_row or same_col:
                    graph[a].append((b, dist))
                    graph[b].append((a, dist))

    return graph

def find_shortest_path(graph: Dict[int, List[Tuple[int, float]]], start: int, end: int) -> List[int]:
    """BFSë¡œ ìµœë‹¨ ê²½ë¡œ ì°¾ê¸°"""
    if start == end:
        return [start]

    queue = deque([(start, [start])])
    visited = {start}

    while queue:
        node, path = queue.popleft()

        for neighbor, _ in graph[node]:
            if neighbor in visited:
                continue

            new_path = path + [neighbor]

            if neighbor == end:
                return new_path

            visited.add(neighbor)
            queue.append((neighbor, new_path))

    return None  # ê²½ë¡œ ì—†ìŒ

def get_turn_waypoints(
    path_nodes: List[int],
    turn_nodes: set
) -> List[int]:
    """ê²½ë¡œì—ì„œ íšŒì „ ë…¸ë“œë§Œ ì¶”ì¶œ (ì‹œì‘/ë í¬í•¨)"""
    waypoints = [path_nodes[0]]  # ì‹œì‘ ë…¸ë“œ

    # ì¤‘ê°„ì˜ íšŒì „ ë…¸ë“œë§Œ ì¶”ê°€
    for node in path_nodes[1:-1]:
        if node in turn_nodes:
            waypoints.append(node)

    waypoints.append(path_nodes[-1])  # ë ë…¸ë“œ
    return waypoints

def interpolate_along_waypoints(
    step_idx: int,
    total_steps: int,
    waypoints: List[int],
    positions: Dict[int, Tuple[float, float]]
) -> Tuple[float, float]:
    """íšŒì „ ë…¸ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ë³„ ì„ í˜• ë³´ê°„"""

    # ê° ì„¸ê·¸ë¨¼íŠ¸(íšŒì „ ë…¸ë“œ ê°„) ê±°ë¦¬ ê³„ì‚° (Manhattan distance)
    segment_lengths = []
    for i in range(len(waypoints) - 1):
        p1 = positions[waypoints[i]]
        p2 = positions[waypoints[i+1]]
        dist = abs(p2[0] - p1[0]) + abs(p2[1] - p1[1])
        segment_lengths.append(dist)

    total_length = sum(segment_lengths)

    # í˜„ì¬ ìŠ¤í…ì˜ ì§„í–‰ë¥ 
    progress = step_idx / (total_steps - 1) if total_steps > 1 else 0.5
    target_dist = progress * total_length

    # ì–´ëŠ ì„¸ê·¸ë¨¼íŠ¸ì— ìˆëŠ”ì§€ ì°¾ê¸°
    cumulative = 0
    for i, seg_len in enumerate(segment_lengths):
        if cumulative + seg_len >= target_dist or i == len(segment_lengths) - 1:
            # ië²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ì— ìˆìŒ (ì„ í˜• ë³´ê°„)
            seg_progress = (target_dist - cumulative) / seg_len if seg_len > 0 else 0.5

            p1 = positions[waypoints[i]]
            p2 = positions[waypoints[i+1]]

            x = p1[0] + seg_progress * (p2[0] - p1[0])
            y = p1[1] + seg_progress * (p2[1] - p1[1])

            return (x, y)

        cumulative += seg_len

    # ëì  ë°˜í™˜
    return positions[waypoints[-1]]

def process_csv_sliding(
    file_path: Path,
    positions: Dict[int, Tuple[float, float]],
    graph: Dict[int, List[Tuple[int, float]]],
    turn_nodes: set,
    feature_mode: str = "mag3",
    window_size: int = 250,
    stride: int = 50,
    debug_count: List[int] = None,
) -> List[Dict]:
    """CSVë¥¼ sliding windowë¡œ ì²˜ë¦¬

    Returns:
        List of samples, each: {"features": [250, n_features], "target": [x, y]}
    """
    with file_path.open() as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    if len(rows) < window_size:
        return []

    # ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ
    try:
        magx = [float(row["MagX"]) for row in rows]
        magy = [float(row["MagY"]) for row in rows]
        magz = [float(row["MagZ"]) for row in rows]
    except (KeyError, ValueError):
        return []

    # Wavelet denoising
    clean_magx = wavelet_denoise(magx)
    clean_magy = wavelet_denoise(magy)
    clean_magz = wavelet_denoise(magz)

    # ê²½ë¡œ ì •ë³´ë¡œ ìœ„ì¹˜ ì–»ê¸°
    parts = file_path.stem.split("_")
    if len(parts) < 2:
        return []

    try:
        start_node = int(parts[0])
        end_node = int(parts[1])
    except ValueError:
        return []

    if start_node not in positions or end_node not in positions:
        return []

    # ìµœë‹¨ ê²½ë¡œ ì°¾ê¸° (BFS)
    path = find_shortest_path(graph, start_node, end_node)

    if path is None:
        # ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        print(f"  âš ï¸  {file_path.name}: ê²½ë¡œ ì—†ìŒ ({start_node}â†’{end_node})")
        return []

    # íšŒì „ ë…¸ë“œë§Œ ì¶”ì¶œ (waypoints)
    waypoints = get_turn_waypoints(path, turn_nodes)

    # ê²½ë¡œ ì •ë³´ ì¶œë ¥ (ë””ë²„ê·¸ìš©, ì²˜ìŒ 10ê°œë§Œ)
    if debug_count is not None and len(waypoints) > 2 and debug_count[0] < 10:
        print(f"  ğŸ›¤ï¸  {file_path.name}: {start_node}â†’{end_node}")
        print(f"      ì „ì²´ ê²½ë¡œ: {path}")
        print(f"      íšŒì „ í¬ì¸íŠ¸: {waypoints} ({len(waypoints)-2}ê°œ íšŒì „)")
        debug_count[0] += 1

    # íšŒì „ ë…¸ë“œ ê¸°ì¤€ ì„¸ê·¸ë¨¼íŠ¸ë³„ ì„ í˜• ë³´ê°„
    num_steps = len(rows)
    positions_list = []
    for i in range(num_steps):
        pos = interpolate_along_waypoints(i, num_steps, waypoints, positions)
        positions_list.append(pos)

    # Adaptive normalization: íŒŒì¼ë³„ í‰ê· /std ê³„ì‚° (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ drift ì²˜ë¦¬)
    magx_mean = np.mean(clean_magx)
    magy_mean = np.mean(clean_magy)
    magz_mean = np.mean(clean_magz)
    magx_std = np.std(clean_magx)
    magy_std = np.std(clean_magy)
    magz_std = np.std(clean_magz)

    # ì•ˆì „ ì¥ì¹˜: stdê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ (ì •ì§€ ìƒíƒœ) ìµœì†Œê°’ ì‚¬ìš©
    magx_std = max(magx_std, 1e-6)
    magy_std = max(magy_std, 1e-6)
    magz_std = max(magz_std, 1e-6)

    # Feature ëª¨ë“œì— ë”°ë¼ íŠ¹ì§• ìƒì„±
    features_list = []
    for i in range(num_steps):
        # Z-score normalization (adaptive)
        magx_norm = (clean_magx[i] - magx_mean) / magx_std
        magy_norm = (clean_magy[i] - magy_mean) / magy_std
        magz_norm = (clean_magz[i] - magz_mean) / magz_std

        if feature_mode == "mag3":
            feat = [magx_norm, magy_norm, magz_norm]
        elif feature_mode == "mag4":
            # Magnitudeë„ adaptiveí•˜ê²Œ
            mag_magnitude = math.sqrt(clean_magx[i]**2 + clean_magy[i]**2 + clean_magz[i]**2)
            mag_array = [math.sqrt(clean_magx[j]**2 + clean_magy[j]**2 + clean_magz[j]**2)
                        for j in range(len(clean_magx))]
            mag_mean = np.mean(mag_array)
            mag_std = max(np.std(mag_array), 1e-6)
            mag_magnitude_norm = (mag_magnitude - mag_mean) / mag_std
            feat = [magx_norm, magy_norm, magz_norm, mag_magnitude_norm]
        elif feature_mode == "full":
            pitch = float(rows[i]["Pitch"])
            roll = float(rows[i]["Roll"])
            yaw = float(rows[i]["Yaw"])
            pitch_norm = pitch / 180.0
            roll_norm = roll / 180.0
            yaw_norm = yaw / 180.0
            feat = [magx_norm, magy_norm, magz_norm, pitch_norm, roll_norm, yaw_norm]
        else:
            raise ValueError(f"Unknown feature_mode: {feature_mode}")

        features_list.append(feat)

    # Sliding window ìƒì„±
    samples = []
    for i in range(0, num_steps - window_size + 1, stride):
        window_features = features_list[i:i + window_size]  # [250, n_features]

        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ìœ„ì¹˜ê°€ label
        last_idx = i + window_size - 1
        last_pos = positions_list[last_idx]
        target = normalize_coord(last_pos[0], last_pos[1])  # (x_norm, y_norm)

        sample = {
            "features": window_features,
            "target": list(target)
        }
        samples.append(sample)

    return samples

def preprocess_sliding(
    raw_dir: Path,
    nodes_path: Path,
    output_dir: Path,
    feature_mode: str = "mag3",
    window_size: int = 250,
    stride: int = 50,
    train_ratio: float = 0.6,
    val_ratio: float = 0.2,
):
    """Sliding window ë°©ì‹ ì „ì²˜ë¦¬"""
    print("=" * 80)
    print("ğŸ”„ Sliding Window ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    print(f"  Feature mode: {feature_mode}")
    print(f"  Window size: {window_size}")
    print(f"  Stride: {stride}")
    print()

    # ë…¸ë“œ ë° ê·¸ë˜í”„
    positions, turn_nodes = read_nodes(nodes_path)
    graph = build_graph(positions)

    print(f"ğŸ”„ íšŒì „ ê°€ëŠ¥ ë…¸ë“œ: {sorted(turn_nodes)}")
    print()

    # ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬ (ê²½ë¡œë³„ë¡œ ê·¸ë£¹í™”)
    path_to_samples = defaultdict(list)
    csv_files = list(raw_dir.glob("*.csv"))

    print(f"ğŸ“‚ ì´ {len(csv_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    print(f"   (ê²½ë¡œ ì°¾ê¸° í™œì„±í™”: íšŒì „ ê²½ë¡œ ìë™ ê°ì§€)\n")

    debug_count = [0]  # mutable counter
    for csv_file in csv_files:
        samples = process_csv_sliding(
            csv_file, positions, graph, turn_nodes, feature_mode, window_size, stride, debug_count
        )
        if samples:
            # ê²½ë¡œ ID ì¶”ì¶œ (íŒŒì¼ëª…: "1_23_0.csv" â†’ path_id: "1_23")
            parts = csv_file.stem.split("_")
            path_id = f"{parts[0]}_{parts[1]}"
            path_to_samples[path_id].extend(samples)

    # ì „ì²´ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    total_samples = sum(len(samples) for samples in path_to_samples.values())
    print(f"âœ… ì´ {total_samples}ê°œ ìƒ˜í”Œ ìƒì„± ({len(path_to_samples)}ê°œ ê²½ë¡œ)")
    print()

    # Train/Val/Test ë¶„í•  (ê²½ë¡œ ê¸°ë°˜ - ì¸µí™” ì¶”ì¶œ)
    print("ğŸ“Š ê²½ë¡œ ê¸°ë°˜ ì¸µí™” ë¶„í•  ìˆ˜í–‰ ì¤‘...")
    paths = list(path_to_samples.keys())
    random.shuffle(paths)

    n_paths = len(paths)
    n_train_paths = int(n_paths * train_ratio)
    n_val_paths = int(n_paths * val_ratio)

    train_paths = paths[:n_train_paths]
    val_paths = paths[n_train_paths:n_train_paths + n_val_paths]
    test_paths = paths[n_train_paths + n_val_paths:]

    # ê° splitì˜ ìƒ˜í”Œ ìˆ˜ì§‘
    train_samples = []
    val_samples = []
    test_samples = []

    for path in train_paths:
        train_samples.extend(path_to_samples[path])
    for path in val_paths:
        val_samples.extend(path_to_samples[path])
    for path in test_paths:
        test_samples.extend(path_to_samples[path])

    print(f"  Train: {len(train_samples)}ê°œ ìƒ˜í”Œ ({len(train_paths)}ê°œ ê²½ë¡œ)")
    print(f"  Val:   {len(val_samples)}ê°œ ìƒ˜í”Œ ({len(val_paths)}ê°œ ê²½ë¡œ)")
    print(f"  Test:  {len(test_samples)}ê°œ ìƒ˜í”Œ ({len(test_paths)}ê°œ ê²½ë¡œ)")
    print()

    # ì €ì¥
    output_dir.mkdir(parents=True, exist_ok=True)

    for split_name, samples in [("train", train_samples), ("val", val_samples), ("test", test_samples)]:
        output_path = output_dir / f"{split_name}.jsonl"
        with output_path.open("w") as f:
            for sample in samples:
                f.write(json.dumps(sample) + "\n")
        print(f"ğŸ’¾ {output_path} ì €ì¥ ì™„ë£Œ")

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    n_features = len(train_samples[0]["features"][0])
    meta = {
        "feature_mode": feature_mode,
        "n_features": n_features,
        "window_size": window_size,
        "stride": stride,
        "n_train": len(train_samples),
        "n_val": len(val_samples),
        "n_test": len(test_samples),
    }

    meta_path = output_dir / "meta.json"
    with meta_path.open("w") as f:
        json.dump(meta, f, indent=2)
    print(f"ğŸ’¾ {meta_path} ì €ì¥ ì™„ë£Œ")

    print()
    print("=" * 80)
    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 80)

    return meta

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--raw-dir", default="data/raw", help="ì›ë³¸ CSV ë””ë ‰í† ë¦¬")
    parser.add_argument("--nodes", default="data/nodes_final.csv")
    parser.add_argument("--output", default="data/sliding", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--feature-mode", default="mag3", choices=["mag3", "mag4", "full"])
    parser.add_argument("--window-size", type=int, default=250)
    parser.add_argument("--stride", type=int, default=50)
    parser.add_argument("--train-ratio", type=float, default=0.6)
    parser.add_argument("--val-ratio", type=float, default=0.2)

    args = parser.parse_args()

    preprocess_sliding(
        raw_dir=Path(args.raw_dir),
        nodes_path=Path(args.nodes),
        output_dir=Path(args.output),
        feature_mode=args.feature_mode,
        window_size=args.window_size,
        stride=args.stride,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
    )
