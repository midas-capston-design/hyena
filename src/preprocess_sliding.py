#!/usr/bin/env python3
"""Sliding Window ë°©ì‹ ì „ì²˜ë¦¬ - Causal Trainingìš©"""
import json
import csv
import math
import random
from pathlib import Path
from typing import Dict, List, Tuple
from collections import deque
import numpy as np
import pywt
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

# Random seed ê³ ì • (ìž¬í˜„ì„±)
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# ì •ê·œí™” ê¸°ì¤€ê°’
BASE_MAG = (-33.0, -15.0, -42.0)
COORD_CENTER = (-44.3, -0.3)
COORD_SCALE = 48.8

def normalize_mag(val: float, base: float) -> float:
    return (val - base) / 10.0

def normalize_coord(x: float, y: float) -> Tuple[float, float]:
    x_norm = (x - COORD_CENTER[0]) / COORD_SCALE
    y_norm = (y - COORD_CENTER[1]) / COORD_SCALE
    return (x_norm, y_norm)

def wavelet_denoise(signal: List[float], wavelet='db4', level=3) -> List[float]:
    """Wavelet denoising (level=3)"""
    coeffs = pywt.wavedec(signal, wavelet, level=level)
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))
    denoised_coeffs = [pywt.threshold(c, uthresh, mode='soft') for c in coeffs]
    return pywt.waverec(denoised_coeffs, wavelet).tolist()

def read_nodes(path: Path) -> Tuple[Dict[int, Tuple[float, float]], set]:
    """ë…¸ë“œ ìœ„ì¹˜ ë° íšŒì „ ë…¸ë“œ ì½ê¸°"""
    positions = {}
    turn_nodes = set()
    with path.open() as f:
        reader = csv.DictReader(f)
        for row in reader:
            node_id = int(row["id"])
            x = float(row["x_m"])
            y = float(row["y_m"])
            node_type = row.get("type", "marker")
            positions[node_id] = (x, y)
            if node_type == "turn":
                turn_nodes.add(node_id)
    return positions, turn_nodes

def build_graph(positions: Dict[int, Tuple[float, float]], connections_path: Path) -> Dict[int, List[Tuple[int, float]]]:
    """ê·¸ëž˜í”„ êµ¬ì¶• - ì—°ê²° íŒŒì¼ ê¸°ë°˜"""
    graph = {node: [] for node in positions}

    # ì—°ê²° íŒŒì¼ ì½ê¸°
    with connections_path.open() as f:
        reader = csv.DictReader(f)
        for row in reader:
            a = int(row["node1"])
            b = int(row["node2"])

            if a not in positions or b not in positions:
                continue

            # Manhattan distance ê³„ì‚°
            xa, ya = positions[a]
            xb, yb = positions[b]
            dist = abs(xb - xa) + abs(yb - ya)

            # ì–‘ë°©í–¥ ì—°ê²°
            graph[a].append((b, dist))
            graph[b].append((a, dist))

    return graph

def find_shortest_path(graph: Dict[int, List[Tuple[int, float]]], start: int, end: int) -> List[int]:
    """BFSë¡œ ìµœë‹¨ ê²½ë¡œ ì°¾ê¸°"""
    if start == end:
        return [start]

    queue = deque([(start, [start])])
    visited = {start}

    while queue:
        node, path = queue.popleft()

        for neighbor, _ in graph[node]:
            if neighbor in visited:
                continue

            new_path = path + [neighbor]

            if neighbor == end:
                return new_path

            visited.add(neighbor)
            queue.append((neighbor, new_path))

    return None  # ê²½ë¡œ ì—†ìŒ

def get_turn_waypoints(
    path_nodes: List[int],
    turn_nodes: set
) -> List[int]:
    """ê²½ë¡œì—ì„œ íšŒì „ ë…¸ë“œë§Œ ì¶”ì¶œ (ì‹œìž‘/ë í¬í•¨)"""
    waypoints = [path_nodes[0]]  # ì‹œìž‘ ë…¸ë“œ

    # ì¤‘ê°„ì˜ íšŒì „ ë…¸ë“œë§Œ ì¶”ê°€
    for node in path_nodes[1:-1]:
        if node in turn_nodes:
            waypoints.append(node)

    waypoints.append(path_nodes[-1])  # ë ë…¸ë“œ
    return waypoints

def interpolate_along_waypoints(
    step_idx: int,
    total_steps: int,
    waypoints: List[int],
    positions: Dict[int, Tuple[float, float]]
) -> Tuple[float, float]:
    """íšŒì „ ë…¸ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ë³„ ì„ í˜• ë³´ê°„"""

    # ê° ì„¸ê·¸ë¨¼íŠ¸(íšŒì „ ë…¸ë“œ ê°„) ê±°ë¦¬ ê³„ì‚° (Manhattan distance)
    segment_lengths = []
    for i in range(len(waypoints) - 1):
        p1 = positions[waypoints[i]]
        p2 = positions[waypoints[i+1]]
        dist = abs(p2[0] - p1[0]) + abs(p2[1] - p1[1])
        segment_lengths.append(dist)

    total_length = sum(segment_lengths)

    # í˜„ìž¬ ìŠ¤í…ì˜ ì§„í–‰ë¥ 
    progress = step_idx / (total_steps - 1) if total_steps > 1 else 0.5
    target_dist = progress * total_length

    # ì–´ëŠ ì„¸ê·¸ë¨¼íŠ¸ì— ìžˆëŠ”ì§€ ì°¾ê¸°
    cumulative = 0
    for i, seg_len in enumerate(segment_lengths):
        if cumulative + seg_len >= target_dist or i == len(segment_lengths) - 1:
            # ië²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ì— ìžˆìŒ (ì„ í˜• ë³´ê°„)
            seg_progress = (target_dist - cumulative) / seg_len if seg_len > 0 else 0.5

            p1 = positions[waypoints[i]]
            p2 = positions[waypoints[i+1]]

            x = p1[0] + seg_progress * (p2[0] - p1[0])
            y = p1[1] + seg_progress * (p2[1] - p1[1])

            return (x, y)

        cumulative += seg_len

    # ëì  ë°˜í™˜
    return positions[waypoints[-1]]

def process_csv_sliding(
    file_path: Path,
    positions: Dict[int, Tuple[float, float]],
    graph: Dict[int, List[Tuple[int, float]]],
    turn_nodes: set,
    feature_mode: str = "mag3",
    window_size: int = 250,
    stride: int = 50,
    wavelet_level: int = 3,
    debug_count: List[int] = None,
) -> List[Dict]:
    """CSVë¥¼ sliding windowë¡œ ì²˜ë¦¬

    Returns:
        List of samples, each: {"features": [250, n_features], "target": [x, y]}
    """
    with file_path.open() as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    if len(rows) < window_size:
        return []

    # ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ
    try:
        magx = [float(row["MagX"]) for row in rows]
        magy = [float(row["MagY"]) for row in rows]
        magz = [float(row["MagZ"]) for row in rows]
    except (KeyError, ValueError):
        return []

    # Wavelet denoising
    clean_magx = wavelet_denoise(magx, level=wavelet_level)
    clean_magy = wavelet_denoise(magy, level=wavelet_level)
    clean_magz = wavelet_denoise(magz, level=wavelet_level)

    # ê²½ë¡œ ì •ë³´ë¡œ ìœ„ì¹˜ ì–»ê¸°
    parts = file_path.stem.split("_")
    if len(parts) < 2:
        return []

    try:
        start_node = int(parts[0])
        end_node = int(parts[1])
    except ValueError:
        return []

    if start_node not in positions or end_node not in positions:
        return []

    # ìµœë‹¨ ê²½ë¡œ ì°¾ê¸° (BFS)
    path = find_shortest_path(graph, start_node, end_node)

    if path is None:
        # ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        print(f"  âš ï¸  {file_path.name}: ê²½ë¡œ ì—†ìŒ ({start_node}â†’{end_node})")
        return []

    # íšŒì „ ë…¸ë“œë§Œ ì¶”ì¶œ (waypoints)
    waypoints = get_turn_waypoints(path, turn_nodes)

    # ê²½ë¡œ ì •ë³´ ì¶œë ¥ (ë””ë²„ê·¸ìš©, ëŒ€í‘œ 1ê°œë§Œ)
    if debug_count is not None and len(waypoints) > 2 and debug_count[0] < 1:
        print(f"  ðŸ›¤ï¸  ëŒ€í‘œ ê²½ë¡œ ì˜ˆì‹œ: {file_path.name} ({start_node}â†’{end_node})")
        print(f"      ì „ì²´ ê²½ë¡œ: {path}")
        print(f"      íšŒì „ í¬ì¸íŠ¸: {waypoints} ({len(waypoints)-2}ê°œ íšŒì „)")
        debug_count[0] += 1

    # íšŒì „ ë…¸ë“œ ê¸°ì¤€ ì„¸ê·¸ë¨¼íŠ¸ë³„ ì„ í˜• ë³´ê°„
    num_steps = len(rows)
    positions_list = []
    for i in range(num_steps):
        pos = interpolate_along_waypoints(i, num_steps, waypoints, positions)
        positions_list.append(pos)

    # Adaptive normalization: íŒŒì¼ë³„ í‰ê· /std ê³„ì‚° (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ drift ì²˜ë¦¬)
    magx_mean = np.mean(clean_magx)
    magy_mean = np.mean(clean_magy)
    magz_mean = np.mean(clean_magz)
    magx_std = np.std(clean_magx)
    magy_std = np.std(clean_magy)
    magz_std = np.std(clean_magz)

    # ì•ˆì „ ìž¥ì¹˜: stdê°€ ë„ˆë¬´ ìž‘ìœ¼ë©´ (ì •ì§€ ìƒíƒœ) ìµœì†Œê°’ ì‚¬ìš©
    magx_std = max(magx_std, 1e-6)
    magy_std = max(magy_std, 1e-6)
    magz_std = max(magz_std, 1e-6)

    # Feature ëª¨ë“œì— ë”°ë¼ íŠ¹ì§• ìƒì„±
    features_list = []
    for i in range(num_steps):
        # Z-score normalization (adaptive)
        magx_norm = (clean_magx[i] - magx_mean) / magx_std
        magy_norm = (clean_magy[i] - magy_mean) / magy_std
        magz_norm = (clean_magz[i] - magz_mean) / magz_std

        if feature_mode == "mag3":
            feat = [magx_norm, magy_norm, magz_norm]
        elif feature_mode == "mag4":
            # Magnitudeë„ adaptiveí•˜ê²Œ
            mag_magnitude = math.sqrt(clean_magx[i]**2 + clean_magy[i]**2 + clean_magz[i]**2)
            mag_array = [math.sqrt(clean_magx[j]**2 + clean_magy[j]**2 + clean_magz[j]**2)
                        for j in range(len(clean_magx))]
            mag_mean = np.mean(mag_array)
            mag_std = max(np.std(mag_array), 1e-6)
            mag_magnitude_norm = (mag_magnitude - mag_mean) / mag_std
            feat = [magx_norm, magy_norm, magz_norm, mag_magnitude_norm]
        elif feature_mode == "full":
            pitch = float(rows[i]["Pitch"])
            roll = float(rows[i]["Roll"])
            yaw = float(rows[i]["Yaw"])
            pitch_norm = pitch / 180.0
            roll_norm = roll / 180.0
            yaw_norm = yaw / 180.0
            feat = [magx_norm, magy_norm, magz_norm, pitch_norm, roll_norm, yaw_norm]
        else:
            raise ValueError(f"Unknown feature_mode: {feature_mode}")

        features_list.append(feat)

    # Sliding window ìƒì„±
    samples = []
    for i in range(0, num_steps - window_size + 1, stride):
        window_features = features_list[i:i + window_size]  # [250, n_features]

        # ë§ˆì§€ë§‰ íƒ€ìž„ìŠ¤í… ìœ„ì¹˜ê°€ label
        last_idx = i + window_size - 1
        last_pos = positions_list[last_idx]
        target = normalize_coord(last_pos[0], last_pos[1])  # (x_norm, y_norm)

        sample = {
            "features": window_features,
            "target": list(target)
        }
        samples.append(sample)

    return samples

def process_csv_wrapper(args):
    """ë©€í‹°í”„ë¡œì„¸ì‹±ìš© ëž˜í¼"""
    csv_file, positions, graph, turn_nodes, feature_mode, window_size, stride, wavelet_level = args
    # ë©€í‹°í”„ë¡œì„¸ì‹±ì—ì„œëŠ” debug ì¶œë ¥ ë”
    return process_csv_sliding(
        csv_file, positions, graph, turn_nodes, feature_mode, window_size, stride, wavelet_level, None
    )

def preprocess_sliding(
    raw_dir: Path,
    nodes_path: Path,
    output_dir: Path,
    feature_mode: str = "mag3",
    window_size: int = 250,
    stride: int = 50,
    train_ratio: float = 0.6,
    val_ratio: float = 0.2,
    wavelet_level: int = 3,
    force: bool = False,
):
    """Sliding window ë°©ì‹ ì „ì²˜ë¦¬

    Args:
        force: Trueë©´ ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ê³  ê°•ì œë¡œ ìž¬ì‹¤í–‰
    """
    print("=" * 80)
    print("ðŸ”„ Sliding Window ì „ì²˜ë¦¬ ì‹œìž‘")
    print("=" * 80)
    print(f"  Feature mode: {feature_mode}")
    print(f"  Window size: {window_size}")
    print(f"  Stride: {stride}")
    print(f"  Wavelet level: {wavelet_level}")
    print()

    # ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
    meta_path = output_dir / "meta.json"
    train_path = output_dir / "train.jsonl"
    val_path = output_dir / "val.jsonl"
    test_path = output_dir / "test.jsonl"

    if not force and meta_path.exists() and train_path.exists() and val_path.exists() and test_path.exists():
        # ë©”íƒ€ë°ì´í„° ë¡œë“œí•˜ì—¬ íŒŒë¼ë¯¸í„° ë¹„êµ
        try:
            with meta_path.open() as f:
                existing_meta = json.load(f)

            # ì£¼ìš” íŒŒë¼ë¯¸í„° ë¹„êµ (ì „ì²˜ë¦¬ì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°)
            params_match = (
                existing_meta.get("seed") == SEED and
                existing_meta.get("feature_mode") == feature_mode and
                existing_meta.get("window_size") == window_size and
                existing_meta.get("stride") == stride and
                existing_meta.get("train_ratio") == train_ratio and
                existing_meta.get("val_ratio") == val_ratio and
                existing_meta.get("wavelet_level") == wavelet_level and
                existing_meta.get("wavelet_type") == "db4" and
                existing_meta.get("base_mag") == list(BASE_MAG) and
                existing_meta.get("coord_center") == list(COORD_CENTER) and
                existing_meta.get("coord_scale") == COORD_SCALE
            )

            if params_match:
                print("âœ… ì „ì²˜ë¦¬ê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
                print(f"   Train: {existing_meta.get('n_train')}ê°œ ìƒ˜í”Œ")
                print(f"   Val:   {existing_meta.get('n_val')}ê°œ ìƒ˜í”Œ")
                print(f"   Test:  {existing_meta.get('n_test')}ê°œ ìƒ˜í”Œ")
                print()
                print("ðŸ’¡ ê°•ì œë¡œ ìž¬ì‹¤í–‰í•˜ë ¤ë©´ --force ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
                print("=" * 80)
                return existing_meta
            else:
                print("âš ï¸  ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ì™€ íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ìž¬ì‹¤í–‰í•©ë‹ˆë‹¤.")
                print(f"   ê¸°ì¡´: feature_mode={existing_meta.get('feature_mode')}, "
                      f"window_size={existing_meta.get('window_size')}, "
                      f"stride={existing_meta.get('stride')}, "
                      f"wavelet_level={existing_meta.get('wavelet_level')}")
                print(f"   ìš”ì²­: feature_mode={feature_mode}, "
                      f"window_size={window_size}, "
                      f"stride={stride}, "
                      f"wavelet_level={wavelet_level}")
                print()
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âš ï¸  ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìž¬ì‹¤í–‰í•©ë‹ˆë‹¤. ({e})")
            print()
    elif force:
        print("ðŸ”¥ ê°•ì œ ìž¬ì‹¤í–‰ ëª¨ë“œ (--force)")
        print()

    # ë…¸ë“œ ë° ê·¸ëž˜í”„
    positions, turn_nodes = read_nodes(nodes_path)
    connections_path = nodes_path.parent / "node_connections.csv"
    graph = build_graph(positions, connections_path)

    print(f"ðŸ”„ íšŒì „ ê°€ëŠ¥ ë…¸ë“œ: {sorted(turn_nodes)}")
    print()

    # ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬ (ë©€í‹°í”„ë¡œì„¸ì‹±)
    all_samples = []
    csv_files = list(raw_dir.glob("*.csv"))

    n_cores = cpu_count()
    print(f"ðŸ“‚ ì´ {len(csv_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘... (CPU ì½”ì–´: {n_cores}ê°œ)")
    print(f"   (ê²½ë¡œ ì°¾ê¸° í™œì„±í™”: íšŒì „ ê²½ë¡œ ìžë™ ê°ì§€)")
    print()

    # ë©€í‹°í”„ë¡œì„¸ì‹± ì¸ìž ì¤€ë¹„
    args_list = [
        (csv_file, positions, graph, turn_nodes, feature_mode, window_size, stride, wavelet_level)
        for csv_file in csv_files
    ]

    # ë³‘ë ¬ ì²˜ë¦¬
    with Pool(processes=n_cores) as pool:
        results = list(tqdm(
            pool.imap(process_csv_wrapper, args_list),
            total=len(csv_files),
            desc="íŒŒì¼ ì²˜ë¦¬ ì¤‘",
            ncols=80,
            unit="file"
        ))

    # ê²°ê³¼ í•©ì¹˜ê¸°
    for samples in results:
        all_samples.extend(samples)

    print(f"\nâœ… ì´ {len(all_samples)}ê°œ ìƒ˜í”Œ ìƒì„±")
    print()

    # Train/Val/Test ë¶„í• 
    random.shuffle(all_samples)

    n_total = len(all_samples)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)

    train_samples = all_samples[:n_train]
    val_samples = all_samples[n_train:n_train + n_val]
    test_samples = all_samples[n_train + n_val:]

    print(f"ðŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"  Train: {len(train_samples)}ê°œ ìƒ˜í”Œ")
    print(f"  Val:   {len(val_samples)}ê°œ ìƒ˜í”Œ")
    print(f"  Test:  {len(test_samples)}ê°œ ìƒ˜í”Œ")
    print()

    # ì €ìž¥
    output_dir.mkdir(parents=True, exist_ok=True)

    for split_name, samples in [("train", train_samples), ("val", val_samples), ("test", test_samples)]:
        output_path = output_dir / f"{split_name}.jsonl"
        with output_path.open("w") as f:
            for sample in samples:
                f.write(json.dumps(sample) + "\n")
        print(f"ðŸ’¾ {output_path} ì €ìž¥ ì™„ë£Œ")

    # ë©”íƒ€ë°ì´í„° ì €ìž¥
    n_features = len(train_samples[0]["features"][0])
    meta = {
        "seed": SEED,
        "feature_mode": feature_mode,
        "n_features": n_features,
        "window_size": window_size,
        "stride": stride,
        "train_ratio": train_ratio,
        "val_ratio": val_ratio,
        "wavelet_level": wavelet_level,
        "wavelet_type": "db4",
        "base_mag": list(BASE_MAG),
        "coord_center": list(COORD_CENTER),
        "coord_scale": COORD_SCALE,
        "n_train": len(train_samples),
        "n_val": len(val_samples),
        "n_test": len(test_samples),
    }

    meta_path = output_dir / "meta.json"
    with meta_path.open("w") as f:
        json.dump(meta, f, indent=2)
    print(f"ðŸ’¾ {meta_path} ì €ìž¥ ì™„ë£Œ")

    print()
    print("=" * 80)
    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 80)

    return meta

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--raw-dir", default="data/raw", help="ì›ë³¸ CSV ë””ë ‰í† ë¦¬")
    parser.add_argument("--nodes", default="data/nodes_final.csv")
    parser.add_argument("--output", default="data/sliding", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--feature-mode", default="mag3", choices=["mag3", "mag4", "full"])
    parser.add_argument("--window-size", type=int, default=250)
    parser.add_argument("--stride", type=int, default=50)
    parser.add_argument("--train-ratio", type=float, default=0.6)
    parser.add_argument("--val-ratio", type=float, default=0.2)
    parser.add_argument("--wavelet-level", type=int, default=3, help="Wavelet denoising level (ê¸°ë³¸: 3)")
    parser.add_argument("--force", action="store_true", help="ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ê³  ê°•ì œë¡œ ìž¬ì‹¤í–‰")

    args = parser.parse_args()

    preprocess_sliding(
        raw_dir=Path(args.raw_dir),
        nodes_path=Path(args.nodes),
        output_dir=Path(args.output),
        feature_mode=args.feature_mode,
        window_size=args.window_size,
        stride=args.stride,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        wavelet_level=args.wavelet_level,
        force=args.force,
    )
