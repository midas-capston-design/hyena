#!/usr/bin/env python3
"""ì „ì²˜ë¦¬ëœ CSV â†’ JSONL ë³€í™˜ (Sliding Window)"""
import json
import csv
import random
from pathlib import Path
from typing import List, Tuple
import numpy as np
import pywt
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

# Random seed ê³ ì •
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# ì •ê·œí™” ê¸°ì¤€ê°’
BASE_MAG = (-33.0, -15.0, -42.0)
COORD_CENTER = (-44.3, -0.3)
COORD_SCALE = 48.8

def normalize_mag(val: float, base: float) -> float:
    return (val - base) / 10.0

def normalize_coord(x: float, y: float) -> Tuple[float, float]:
    x_norm = (x - COORD_CENTER[0]) / COORD_SCALE
    y_norm = (y - COORD_CENTER[1]) / COORD_SCALE
    return (x_norm, y_norm)

def wavelet_denoise(signal: List[float], wavelet='db4', level=3) -> List[float]:
    """Wavelet denoising"""
    if len(signal) < 2**level:
        return signal
    coeffs = pywt.wavedec(signal, wavelet, level=level)
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))
    denoised_coeffs = [pywt.threshold(c, uthresh, mode='soft') for c in coeffs]
    return pywt.waverec(denoised_coeffs, wavelet).tolist()

def process_file(args):
    """íŒŒì¼ í•˜ë‚˜ ì²˜ë¦¬"""
    file_path, window_size, stride = args

    # CSV ì½ê¸°
    with file_path.open() as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    if len(rows) < window_size:
        return []

    # ì‹ í˜¸ ì¶”ì¶œ ë° ì›¨ì´ë¸Œë › ë””ë…¸ì´ì§•
    magx = [float(row['magx']) for row in rows]
    magy = [float(row['magy']) for row in rows]
    magz = [float(row['magz']) for row in rows]

    magx_denoised = wavelet_denoise(magx)
    magy_denoised = wavelet_denoise(magy)
    magz_denoised = wavelet_denoise(magz)

    # Sliding window ìƒì„±
    samples = []
    for i in range(0, len(rows) - window_size + 1, stride):
        window_rows = rows[i:i + window_size]

        # Features: ì •ê·œí™”ëœ ì„¼ì„œê°’
        features = []
        for j, row in enumerate(window_rows):
            idx = i + j
            feature_vec = [
                normalize_mag(magx_denoised[idx], BASE_MAG[0]),
                normalize_mag(magy_denoised[idx], BASE_MAG[1]),
                normalize_mag(magz_denoised[idx], BASE_MAG[2]),
                float(row['yaw']) / 180.0,
                float(row['roll']) / 180.0,
                float(row['pitch']) / 180.0,
            ]
            features.append(feature_vec)

        # Target: ìœˆë„ìš° ëì ì˜ ì •ê·œí™”ëœ ì¢Œí‘œ
        last_row = window_rows[-1]
        x = float(last_row['x'])
        y = float(last_row['y'])
        x_norm, y_norm = normalize_coord(x, y)

        sample = {
            "features": features,
            "target": [x_norm, y_norm]
        }
        samples.append(sample)

    return samples

def main():
    # ì„¤ì •
    preprocessed_dir = Path("data/preprocessed")
    output_dir = Path("data/sliding_mag4")
    output_dir.mkdir(exist_ok=True, parents=True)

    window_size = 250
    stride = 25

    print("=" * 80)
    print("ì „ì²˜ë¦¬ëœ CSV â†’ JSONL ë³€í™˜")
    print("=" * 80)
    print(f"ìž…ë ¥ ë””ë ‰í† ë¦¬: {preprocessed_dir}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ìœˆë„ìš° í¬ê¸°: {window_size}")
    print(f"ìŠ¤íŠ¸ë¼ì´ë“œ: {stride}")
    print()

    # ìºì‹±: ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
    meta_path = output_dir / "meta.json"
    train_path = output_dir / "train.jsonl"
    val_path = output_dir / "val.jsonl"
    test_path = output_dir / "test.jsonl"

    if meta_path.exists() and train_path.exists() and val_path.exists() and test_path.exists():
        try:
            with meta_path.open() as f:
                existing_meta = json.load(f)

            # íŒŒë¼ë¯¸í„° ë¹„êµ
            params_match = (
                existing_meta.get("window_size") == window_size and
                existing_meta.get("stride") == stride and
                existing_meta.get("n_features") == 6
            )

            if params_match:
                print("âœ… ì „ì²˜ë¦¬ê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
                print(f"   Train: {existing_meta.get('n_train')}ê°œ ìƒ˜í”Œ")
                print(f"   Val:   {existing_meta.get('n_val')}ê°œ ìƒ˜í”Œ")
                print(f"   Test:  {existing_meta.get('n_test')}ê°œ ìƒ˜í”Œ")
                print()
                print("ðŸ’¡ ê°•ì œë¡œ ìž¬ì‹¤í–‰í•˜ë ¤ë©´ meta.jsonì„ ì‚­ì œí•˜ì„¸ìš”.")
                print("=" * 80)
                return
            else:
                print("âš ï¸  ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ì™€ íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ìž¬ì‹¤í–‰í•©ë‹ˆë‹¤.")
                print(f"   ê¸°ì¡´: window_size={existing_meta.get('window_size')}, stride={existing_meta.get('stride')}")
                print(f"   ìš”ì²­: window_size={window_size}, stride={stride}")
                print()
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âš ï¸  ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìž¬ì‹¤í–‰í•©ë‹ˆë‹¤. ({e})")
            print()

    # ëª¨ë“  CSV íŒŒì¼
    csv_files = sorted(preprocessed_dir.glob("*.csv"))
    print(f"ì´ {len(csv_files)}ê°œ íŒŒì¼ ë°œê²¬")

    # Train/Val/Test ë¶„í•  (8:1:1)
    random.shuffle(csv_files)
    n_train = int(len(csv_files) * 0.8)
    n_val = int(len(csv_files) * 0.1)

    train_files = csv_files[:n_train]
    val_files = csv_files[n_train:n_train + n_val]
    test_files = csv_files[n_train + n_val:]

    print(f"Train: {len(train_files)}ê°œ")
    print(f"Val:   {len(val_files)}ê°œ")
    print(f"Test:  {len(test_files)}ê°œ")
    print()

    # ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ì²˜ë¦¬
    n_workers = min(cpu_count(), 8)
    print(f"ë³‘ë ¬ ì²˜ë¦¬: {n_workers} workers\n")

    def process_split(files, split_name):
        print(f"ì²˜ë¦¬ ì¤‘: {split_name}")

        args_list = [(f, window_size, stride) for f in files]

        with Pool(n_workers) as pool:
            results = list(tqdm(
                pool.imap(process_file, args_list),
                total=len(files),
                desc=split_name
            ))

        # ìƒ˜í”Œ ìˆ˜ì§‘
        all_samples = []
        for samples in results:
            all_samples.extend(samples)

        # JSONL ì €ìž¥
        output_file = output_dir / f"{split_name}.jsonl"
        with output_file.open('w') as f:
            for sample in all_samples:
                f.write(json.dumps(sample) + '\n')

        print(f"  {split_name}: {len(all_samples)}ê°œ ìƒ˜í”Œ ì €ìž¥ â†’ {output_file}")
        return len(all_samples)

    # ê° split ì²˜ë¦¬
    n_train_samples = process_split(train_files, "train")
    n_val_samples = process_split(val_files, "val")
    n_test_samples = process_split(test_files, "test")

    # ë©”íƒ€ë°ì´í„° ì €ìž¥
    meta = {
        "n_features": 6,  # magx, magy, magz, yaw, roll, pitch
        "window_size": window_size,
        "stride": stride,
        "n_train": n_train_samples,
        "n_val": n_val_samples,
        "n_test": n_test_samples,
    }

    with (output_dir / "meta.json").open('w') as f:
        json.dump(meta, f, indent=2)

    print()
    print("=" * 80)
    print("âœ… ë³€í™˜ ì™„ë£Œ!")
    print(f"  ì¶œë ¥: {output_dir}")
    print(f"  Train: {n_train_samples:,}ê°œ ìƒ˜í”Œ")
    print(f"  Val:   {n_val_samples:,}ê°œ ìƒ˜í”Œ")
    print(f"  Test:  {n_test_samples:,}ê°œ ìƒ˜í”Œ")
    print("=" * 80)

if __name__ == "__main__":
    main()
